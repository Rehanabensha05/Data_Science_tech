# -*- coding: utf-8 -*-
"""twitter_prodigy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16I6IZL7LhEtwKhrCfcdrEDeH0G_V1KQ2
"""

#import libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

twitter=pd.read_csv("/content/twitter_training.csv")

twitter.head()

twitter.tail()

twitter.shape

twitter.info()

twitter.describe()

twitter.isnull().sum()

twit=twitter.dropna()

twit.isnull().sum()

twi=twit.rename(columns={'2401':'Id','Borderlands':'Entity','Positive':'Sentiment','im getting on borderlands and i will murder you all ,':'Comment'})

twi.head()

twi.nunique()

twi['Sentiment'].value_counts()

sns.countplot(x='Sentiment',data=twi,hue='Sentiment')
plt.title('Sentiment Count')
plt.show()

plt.figure(figsize=(20,5))
sns.countplot(x='Entity',data=twi,hue='Sentiment')
plt.xticks(rotation=90)
plt.show()

plt.pie(x=twi['Sentiment'].value_counts().values,labels=twi['Sentiment'].value_counts().index,autopct='%.1f%%',explode=[0.03,0.03,0.03,0.03])
plt.title("Sentiment Ratio")
plt.show()

import re
def review(twi):
    twi = re.sub('http\S+\s*', ' ',twi)  # remove URLs
    twi= re.sub('RT|cc', ' ',twi)  # remove RT and cc
    twi= re.sub('#\S+', '',twi)  # remove hashtags
    twi= re.sub('@\S+', '  ', twi)  # remove mentions
    twi= re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', twi)  # remove punctuations
    twi= re.sub(r'[^\x00-\x7f]',r' ',twi)
    twi= re.sub('\s+', ' ',twi)  # remove extra whitespace
    return twi

twi['Comment'] = twi.Comment.apply(lambda x: review(x))

import nltk
from nltk.corpus import stopwords
import string
nltk.download('stopwords')#remove stopwords
nltk.download('punkt')#remove punctuation

oneSetOfStopWords = set(stopwords.words('english')+['``',"''"])
totalWords =[]
Sentences = twi['Comment'].values
cleanedSentences = ""
for records in Sentences:
    cleanedText = review(records)
    cleanedSentences += cleanedText
    requiredWords = nltk.word_tokenize(cleanedText)
    for word in requiredWords:
        if word not in oneSetOfStopWords and word not in string.punctuation:
            totalWords.append(word)

#display frequency of the words
wordfreqdist = nltk.FreqDist(totalWords)
mostcommon = wordfreqdist.most_common(50)
print(mostcommon)

#word_cloud
from wordcloud import WordCloud

# Create a word cloud of the most frequent words in the positive reviews
positive_words = ' '.join(twi[twi['Sentiment'] == 'Positive']['Comment'].tolist())
wordcloud = WordCloud(width=800, height=600, background_color='black').generate(positive_words)

# Plot the word cloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Positive Reviews')
plt.show()

# Create a word cloud of the most frequent words in the Negative reviews
negative_words = ' '.join(twi[twi['Sentiment'] == 'Negative']['Comment'].tolist())
wordcloud = WordCloud(width=800, height=600, background_color='black').generate(negative_words)

# Plot the word cloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Negative Reviews')
plt.show()

# Create a word cloud of the most frequent words in the positive reviews
neutral_words = ' '.join(twi[twi['Sentiment'] == 'Neutral']['Comment'].tolist())
wordcloud = WordCloud(width=800, height=600, background_color='black').generate(neutral_words)

# Plot the word cloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Neutral Reviews')
plt.show()

# Create a word cloud of the most frequent words in the positive reviews
irrelevant_words = ' '.join(twi[twi['Sentiment'] == 'Irrelevant']['Comment'].tolist())
wordcloud = WordCloud(width=800, height=600, background_color='black').generate(irrelevant_words)

# Plot the word cloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Irrelatvent Reviews')
plt.show()

#sentimental analysis using naive bayes
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(twi['Comment'], twi['Sentiment'], test_size=0.2, random_state=42)


# CountVectorizer is used to convert text data into a matrix of token counts.
vectorizer = CountVectorizer()

X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# Train the Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_vect, y_train)

# Make predictions on the test set
y_pred = nb_classifier.predict(X_test_vect)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)